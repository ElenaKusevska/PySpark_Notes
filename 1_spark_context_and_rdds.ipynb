{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67c960a7-d6c6-4da5-8af1-550ac826dffd",
   "metadata": {},
   "source": [
    "## Spark Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a7a3d4-cf45-422c-806c-46419a85334d",
   "metadata": {},
   "source": [
    "Spark Context is the original entrypoint to Spark applications in older versions of Spark. It stores information like cluster details and cluster connection and handles the rdd API.\n",
    "\n",
    "Spark Session is the current entry point for Spark applications, from Spark 2.0 onwards. It is a combined entrypoint for all of Spark's functionalities, and so it also contains a Spark Context. It is initialized with the Spark Session Builder. It offers dataframe and SparkSQL functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28a8c095-70d2-426a-8ef1-5a9dc5194e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bf04d49-caf7-4d9e-bae6-fe1efd07ee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"ContextExample\").setMaster(\"local[*]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "423b5096-c6c4-4f06-aa56-bfcf1c3d63ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/31 00:21:50 WARN Utils: Your hostname, elena-VB resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "24/03/31 00:21:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/31 00:21:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc=SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6710bb-013a-4f68-acbc-558a2e2f2ce0",
   "metadata": {},
   "source": [
    "## RDD - Resilient Distributed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5570fbbe-10a2-43df-8158-50d7a3fa6a0e",
   "metadata": {},
   "source": [
    "Fundamental Data Structure in Spark, providing fault-tolerant and parallelized data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6620f6b3-99eb-44cb-9ce5-acb72a20e86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = range(1, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42274760-015e-40ab-b2c8-be804e1bc788",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7317ff90-008e-47b5-8e51-d280c6b5407f",
   "metadata": {},
   "source": [
    "A transformation is an operation on an RDD that results in a new RDD. An operation/transformation is quick because lazy evaluation is performed, so the calculation is not carried out when the new RDD is generated. The RDD will contain a series of transformations, or computation instructions, that will only be carried out when an action is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73d48cb4-153a-4dc6-bb3c-8b6b2ce26e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_squared = rdd.map(lambda x: x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ff7b74c-9048-4e80-8612-46f79606efa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rdd_squared.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7bf60e71-16f3-432b-82f7-860e1841adfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 25]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fbb8c8a8-4a55-4071-a213-8b5b257bfae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_filtered = rdd_squared.filter(lambda x : x<10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3a1feab0-4dfe-4257-b533-87828f9e67c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filtered_count = rdd_filtered.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "543a61af-3ec4-4c26-a335-df052eb7f485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c5fb050-a46a-4168-87ca-baa43dfe0383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SparkContext.stop of <SparkContext master=local[*] appName=ContextExample>>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e7fae4-9d36-4b32-9cc0-f6f9aa8ba84e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
